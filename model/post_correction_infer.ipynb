{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('phungqv': conda)"
  },
  "interpreter": {
   "hash": "f251317cbd741716559e4de373cc4436450ae79b12f47fee4349c7f6a5b242a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoConfig, PhobertTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('/home3/phungqv/post_correction/vietnam_number')\n",
    "from vietnam_number import w2n, w2n_single\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PhobertTokenizer.from_pretrained('/home3/phungqv/post_correction/phobert_tokenizer')\n",
    "# pretrained = AutoModelForSeq2SeqLM.from_pretrained('/home3/phungqv/data_generate/text-normalization/checkpoint-84561')\n",
    "pretrained = AutoModelForSeq2SeqLM.from_pretrained('/home3/phungqv/post_correction/model/checkpoint/word2num/checkpoint-153441')\n",
    "device = \"cuda:0\"\n",
    "pretrained = pretrained.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_year_with_month = re.compile(r'((ngày|mùng)\\s)?(?(1)((mười|mươi|một|mốt|hai|ba|bốn|tư|năm|lăm|sáu|bảy|bẩy|tám|chín)(\\s)?){1,2})(tháng\\s)(?(6)((một|hai|ba|tư|bốn|năm|sáu|bảy|bẩy|tám|chín|mười|mười một|mười hai)\\s))năm', re.IGNORECASE)\n",
    "\n",
    "pattern_num = re.compile(r'((không\\s?)*(một|hai|ba|bốn|tư|năm|sáu|bảy|bẩy|tám|chín|mười)\\s?)((linh|lẻ|không|một|mốt|hai|ba|bốn|tư|năm|lăm|nhăm|sáu|bảy|bẩy|tám|chín|mười|mươi|chục|trăm|nghìn|ngàn|triệu|tỉ|tỷ)\\s?)*(?!#)', re.IGNORECASE)\n",
    "\n",
    "pattern_more1num = re.compile(r'((không\\s)*(một|hai|ba|bốn|tư|năm|sáu|bảy|bẩy|tám|chín|mười)\\s?)((linh|lẻ|không|một|mốt|hai|ba|bốn|tư|năm|lăm|nhăm|sáu|bảy|bẩy|tám|chín|mười|mươi|chục|trăm|nghìn|ngàn|triệu|tỉ|tỷ)\\s?)+', re.IGNORECASE)\n",
    "\n",
    "pattern_year_only_full_num = re.compile(r'(một|hai|ba|bốn|năm|sáu|bảy|bẩy|tám|chín|mười)\\s(mốt|lăm|tư|mươi|linh|lẻ|chục|trăm|nghìn|triệu|tỉ|tỷ)', re.IGNORECASE)\n",
    "\n",
    "pattern_num_with_phay = re.compile(r'((không\\s)*(một|hai|ba|bốn|tư|năm|sáu|bảy|bẩy|tám|chín|mười)\\s?)((linh|lẻ|không|một|mốt|hai|ba|bốn|tư|năm|lăm|nhăm|sáu|bảy|bẩy|tám|chín|mười|mươi|chục|trăm|nghìn|ngàn|triệu|tỉ|tỷ)\\s?)*,((không|một|hai|ba|bốn|năm|sáu|bảy|bẩy|tám|chín)\\s?)+', re.IGNORECASE)\n",
    "\n",
    "\n",
    "full_num_word_list = ['mươi', 'mười','linh','lẻ','chục','trăm','nghìn','ngàn','triệu','tỉ','tỷ','lăm','nhăm']\n",
    "\n",
    "def check_single_num(num):\n",
    "    for word in full_num_word_list:\n",
    "        if word in num:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def num_with_phay_process(match):\n",
    "    whole_num = match.group(0)\n",
    "    num_list = whole_num.strip().split(',')\n",
    "    new_num_list = []\n",
    "    for num in num_list:\n",
    "        if check_single_num(num):\n",
    "            new_num_list.append(str(w2n_single(num)))\n",
    "        else:\n",
    "            new_num_list.append(str(w2n(num)))\n",
    "    return ','.join(new_num_list) + ' '\n",
    "\n",
    "\n",
    "def num_process(match):\n",
    "    whole_num = match.group(0)\n",
    "    year = ''\n",
    "    if whole_num.startswith('năm'):\n",
    "        if check_year_only_full_num(whole_num):\n",
    "            year = '##' + whole_num[:3] + '##' + ' ' \n",
    "            whole_num = whole_num[3:]\n",
    "    # else:\n",
    "    if check_single_num(whole_num):\n",
    "        return year + str(w2n_single(whole_num)) + ' '\n",
    "    else:\n",
    "        return year + str(w2n(whole_num)) + ' '\n",
    "\n",
    "\n",
    "def year_with_month_num_process(match):\n",
    "    num = match.group(0).strip()\n",
    "    if check_single_num(num):\n",
    "        return str(w2n_single(num)) + ' '\n",
    "    else:\n",
    "        return str(w2n(num)) + ' '\n",
    "\n",
    "def year_with_month_process(match):\n",
    "    result = match.group(0)[:-3] + '##' + match.group(0)[-3:] + '##'\n",
    "    return re.sub(pattern_num, year_with_month_num_process, result)\n",
    "    \n",
    "\n",
    "def check_year_only_full_num(s):\n",
    "    if not s.startswith('năm'):\n",
    "        # print('doesnt start with nam')\n",
    "        return False\n",
    "    remove_nam = s[4:]\n",
    "    for word in full_num_word_list:\n",
    "        if remove_nam.startswith(word):\n",
    "            return False\n",
    "    result = re.search(pattern_year_only_full_num, s)\n",
    "    if result:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "number = re.compile(r'(0|1|2|3|4|5|6|7|7|8|9){2,}')\n",
    "\n",
    "def add_space_to_num(match):\n",
    "    str_l = [char for char in match.group(0)]\n",
    "    return ' '.join(str_l)\n",
    "\n",
    "\n",
    "def convert_num(sentence):\n",
    "    sentence = re.sub(pattern_year_with_month, year_with_month_process, sentence)\n",
    "    sentence = re.sub(pattern_num_with_phay, num_with_phay_process, sentence)\n",
    "    # print(sentence)\n",
    "    sentence = re.sub(pattern_more1num, num_process, sentence)\n",
    "    sentence = re.sub(number, add_space_to_num, sentence)\n",
    "    sentence = sentence.replace('##', '')\n",
    "    sentence = ' '.join(sentence.split())\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "năm hai ba|5 2 3\nba,năm triệu|3,5 triệu\nhai không hai mốt|2 0 2 1\nsđt không chín Bảy chín một tám Hai sáu Hai tám alo|sđt 0 9 7 9 1 8 2 6 2 8 alo\nngày hai lăm tháng mười hai năm hai không hai mốt|ngày 2 5 tháng 1 2 năm 2 0 2 1\nvào năm hai không hai mốt|vào năm 2 0 2 1\nhai trăm linh hai|2 0 2\nnăm trăm lẻ một|5 0 1\nhay năm hai không hai mốt|hay năm 2 0 2 1\nnăm nghìn không trăm lẻ bảy|5 0 0 7\nngày mùng một tháng một năm hai nghìn|ngày mùng 1 tháng 1 năm 2 0 0 0\n"
     ]
    }
   ],
   "source": [
    "test_text = [\n",
    "    'năm hai ba',\n",
    "    'ba,năm triệu',\n",
    "    'hai không hai mốt',\n",
    "    'sđt không chín Bảy chín một tám Hai sáu Hai tám alo',\n",
    "    'ngày hai lăm tháng mười hai năm hai không hai mốt',\n",
    "    'vào năm hai không hai mốt',\n",
    "    'hai trăm linh hai',\n",
    "    'năm trăm lẻ một',\n",
    "    'hay năm hai không hai mốt',\n",
    "    'năm nghìn không trăm lẻ bảy',\n",
    "    'ngày mùng một tháng một năm hai nghìn'\n",
    "]\n",
    "for sentence in test_text:\n",
    "    print(sentence, convert_num(sentence), sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ngày mùng 1 tháng 1 năm 2 0 0 0\n[[0, 0, 43, 16, 16, 76, 2005, 2005, 2005, 2]]\n['<s> <s> ngày một một 2 0 0 0 </s>']\nmodel output: ngày một một 2 0 0 0\nmodel time: 0.16053175926208496\ninput: ngày mùng 1 tháng 1 năm 2 0 0 0\noutput: ngày một một 2 0 0 0\ntotal time taken: 0.16091394424438477\n"
     ]
    }
   ],
   "source": [
    "sentence = 'ngày mùng một tháng một năm hai nghìn'\n",
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    sentence = convert_num(sentence)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "print(sentence)\n",
    "# model i choose you!\n",
    "inputs = tokenizer.encode(sentence, return_tensors='pt')\n",
    "inputs = inputs.to(device)\n",
    "outputs = pretrained.generate(inputs, max_length=128, num_beams=6, early_stopping=True)\n",
    "\n",
    "outputs = outputs.tolist()\n",
    "print(outputs)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "output_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(f'model output: {output_sentence}')\n",
    "print('model time: {}'.format(time.time() - start))\n",
    "\n",
    "# rule tiem\n",
    "output_sentence = output_sentence.replace('đô la', '$')\n",
    "output_sentence = output_sentence.replace(' phẩy ', ',')\n",
    "output_sentence = output_sentence.replace(' phần trăm', '%')\n",
    "output_sentence = output_sentence.replace(' a còng', '@')\n",
    "# try:\n",
    "#     output_sentence = convert_num(output_sentence)\n",
    "# except Exception as e:\n",
    "#     print(str(e))\n",
    "print(f'input: {sentence}')\n",
    "print(f'output: {output_sentence}')\n",
    "print('total time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "218it [01:35,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_list = []\n",
    "with open('input_ASR.txt') as f:\n",
    "    for sentence in tqdm(f):\n",
    "        sentence = sentence.strip()\n",
    "        inputs = tokenizer.encode(sentence, return_tensors='pt')\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = pretrained.generate(inputs, max_length=128, num_beams=6, early_stopping=True)\n",
    "\n",
    "        outputs = outputs.tolist()\n",
    "        sentence_list.append(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_asr.txt', 'w') as new_f:\n",
    "    for sentence in sentence_list:\n",
    "        new_f.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = PhobertTokenizer.from_pretrained('/home3/phungqv/phobert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 12118, 2]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tokenizer.encode('khủng_long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1 0 0 %'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "tokenizer.decode(2138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['<s> <s> mình muốn mở mới thẻ thì mình làm như thế nào nhở </s>']\nmình muốn mở mới thẻ thì mình làm như thế nào nhở\n"
     ]
    }
   ],
   "source": [
    "sentence = 'mình muốn mở mới thẻ thì mình làm như thế nào nhở'\n",
    "\n",
    "inputs = tmp.encode(sentence, return_tensors='pt')\n",
    "inputs = inputs.to(device)\n",
    "outputs = pretrained.generate(inputs, max_length=128, num_beams=6, early_stopping=True)\n",
    "\n",
    "\n",
    "outputs = outputs.tolist()\n",
    "print(tmp.batch_decode(outputs))\n",
    "print(tmp.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}