{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('phungqv': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f251317cbd741716559e4de373cc4436450ae79b12f47fee4349c7f6a5b242a1"
   }
  },
  "interpreter": {
   "hash": "f251317cbd741716559e4de373cc4436450ae79b12f47fee4349c7f6a5b242a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoConfig\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'Helsinki-NLP/opus-mt-vi-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "config = AutoConfig.from_pretrained('/home3/phungqv/data_generate/config.json')\n",
    "model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-5df29c75ee5cfba2\n",
      "Reusing dataset csv (/home/phungqv/.cache/huggingface/datasets/csv/default-5df29c75ee5cfba2/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset('text', data_files={'train' : '/home3/phungqv/data_generate/transcript.txt', 'val' : '/home3/phungqv/data_generate/transcript.txt'})\n",
    "dataset = load_dataset('csv', data_files={'train' : '/home3/phungqv/data_generate/data/train.csv', \n",
    "'val' : '/home3/phungqv/data_generate/data/val.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"sacrebleu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'target'],\n",
       "        num_rows: 76167\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input', 'target'],\n",
       "        num_rows: 512\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input': 'từ cách vận hành vũ khí tới chiền thuật sử dụng con người nuôn nà qyết định',\n",
       " 'target': 'từ cách vận hành vũ khí tới chiến thuật sử dụng con người luôn là quyết định'}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples['input']]\n",
    "    targets = [ex for ex in examples['target']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "    # print(model_inputs)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # print(model_inputs)\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/phungqv/.cache/huggingface/datasets/csv/default-5df29c75ee5cfba2/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-1caec707f2a89e40.arrow\n",
      "Loading cached processed dataset at /home/phungqv/.cache/huggingface/datasets/csv/default-5df29c75ee5cfba2/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-0b025244766861fe.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"text-normalization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    # learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # weight_decay=0.01,\n",
    "    save_total_limit=15,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_dir='/home3/phungqv/data_generate/logs',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='3573' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/3573 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3573, training_loss=4.947500814485377, metrics={'train_runtime': 551.8494, 'train_samples_per_second': 6.475, 'total_flos': 0, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('/home3/phungqv/data_generate/checkpoint/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cc53fc0041ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2c02a511f728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'truyển đổi số'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "inputs = tokenizer.encode('truyển đổi số', return_tensors='pt')\n",
    "inputs = inputs.to(device)\n",
    "outputs = model.generate(inputs, max_length=40, num_beams=6, early_stopping=True)\n",
    "\n",
    "\n",
    "outputs = outputs.tolist()\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [1, 0, 0, 0],\n",
       " 'precisions': [100.0, 0.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 1,\n",
       " 'ref_len': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "fake_preds = [\"một\"]\n",
    "fake_labels = [[\"một\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1086, 10573, 10013, 2], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# vn_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer('N@@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['S e a s @ @']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "tokenizer.batch_decode([53738])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained = AutoModelForSeq2SeqLM.from_pretrained('/home3/phungqv/data_generate/text-normalization-fintune/checkpoint-13146')\n",
    "pretrained = AutoModelForSeq2SeqLM.from_pretrained('/home3/phungqv/data_generate/text-normalization/checkpoint-84561')\n",
    "device = \"cuda:0\"\n",
    "pretrained = pretrained.to(device)\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['<s> <s> ủy ban nhân dân </s>']\nủy ban nhân dân\n--- 0.2122957706451416 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "inputs = tokenizer.encode('', return_tensors='pt')\n",
    "inputs = inputs.to(device)\n",
    "outputs = pretrained.generate(inputs, max_length=128, num_beams=6, early_stopping=True)\n",
    "\n",
    "\n",
    "outputs = outputs.tolist()\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = ['tiến hanh truyển đổi xố',\n",
    "'ngận hàng bi ai đi vi',\n",
    "'dịch vụ vê xê bê pây của ngân hàng việt com banh',\n",
    "'vi xi bi',\n",
    "'in tơ nét banh kinh',\n",
    "'bảy trăm bốn mốt',\n",
    "'việt kiều ốt xờ trây li a',\n",
    "'tài khoản phây búc hơn nghìn lai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input:  tiến hanh truyển đổi xố\n",
      "output: tiến hành chuyển đổi số\n",
      "-----------\n",
      "input:  ngận hàng bi ai đi vi\n",
      "output: ngân hàng bidv\n",
      "-----------\n",
      "input:  dịch vụ vê xê bê pây của ngân hàng việt com banh\n",
      "output: dịch vụ vcb của ngân hàng vietcombank\n",
      "-----------\n",
      "input:  vi xi bi\n",
      "output: vcbi\n",
      "-----------\n",
      "input:  in tơ nét banh kinh\n",
      "output: internet bank\n",
      "-----------\n",
      "input:  bảy trăm bốn mốt\n",
      "output: bảy 400\n",
      "-----------\n",
      "input:  việt kiều ốt xờ trây li a\n",
      "output: việt kiều australia\n",
      "-----------\n",
      "input:  tài khoản phây búc hơn nghìn lai\n",
      "output: tài khoản facebook hơn 2000\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for s in sentence_list:\n",
    "    inputs = tokenizer.encode(s, return_tensors='pt')\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = pretrained.generate(inputs, max_length=128, num_beams=6, early_stopping=True)\n",
    "\n",
    "\n",
    "    outputs = outputs.tolist()\n",
    "    print('input:  ' + s)\n",
    "    print('output: ' + tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "    print('-----------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home3/congpt/asr/crawl_data/infer_youtube_remove.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                path  \\\n",
       "0  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "1  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "2  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "3  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "4  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "5  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "6  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "7  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "8  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "9  /home3/huydd/cut_audio_post_process/newCrawl_s...   \n",
       "\n",
       "                                          transcript  infer_time  \n",
       "0                                            cắt tay    5.129343  \n",
       "1  bị bạo lực học đường ngày nào bị lạm dụng tình...    7.063390  \n",
       "2                                    Có lẽ là tất cả   12.315212  \n",
       "3                            hoặc là về giá cổ phiếu    5.819670  \n",
       "4  giao lưu tại các trường ở nước ngoài đó hay kh...    5.151469  \n",
       "5  và cái chỗ đó thì mình cảm giác như là như một...    7.100409  \n",
       "6  Anh ơi bản audio và led trên spotify và Apple ...   14.590879  \n",
       "7  mấy tháng sau khi con đã cứng cáp cả nhà về ch...   14.616554  \n",
       "8                                                cái    4.604540  \n",
       "9  Dạ có người phụ nữ Nó cũng có điều kiện con là...    6.568739  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>transcript</th>\n      <th>infer_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>cắt tay</td>\n      <td>5.129343</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>bị bạo lực học đường ngày nào bị lạm dụng tình...</td>\n      <td>7.063390</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>Có lẽ là tất cả</td>\n      <td>12.315212</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>hoặc là về giá cổ phiếu</td>\n      <td>5.819670</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>giao lưu tại các trường ở nước ngoài đó hay kh...</td>\n      <td>5.151469</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>và cái chỗ đó thì mình cảm giác như là như một...</td>\n      <td>7.100409</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>Anh ơi bản audio và led trên spotify và Apple ...</td>\n      <td>14.590879</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>mấy tháng sau khi con đã cứng cáp cả nhà về ch...</td>\n      <td>14.616554</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>cái</td>\n      <td>4.604540</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/home3/huydd/cut_audio_post_process/newCrawl_s...</td>\n      <td>Dạ có người phụ nữ Nó cũng có điều kiện con là...</td>\n      <td>6.568739</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home3/phungqv/anhcong.txt', 'w') as f:\n",
    "    for sentence in df.transcript:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}